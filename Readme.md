# üöóüìä ETL Pipeline pour le Data Warehouse de Location de V√©hicules

Ce projet met en ≈ìuvre une cha√Æne ETL compl√®te pour extraire des donn√©es depuis une base de donn√©es PostgreSQL, les transformer selon un sch√©ma en √©toile (star schema) et les charger dans Snowflake. Le pipeline suit une architecture en **m√©dallion** avec trois niveaux‚ÄØ: **Bronze**, **Silver** et **Gold**, garantissant la tra√ßabilit√© et la qualit√© des donn√©es.


## Table des Mati√®res

| Section                   | Description                                        |
| ------------------------- | -------------------------------------------------- |
| [Introduction](#introduction)      | Contexte et objectifs du projet                  |
| [Architecture](#architecture)      | Sch√©ma en √©toile et architecture en m√©dallion    |
| [Technologies](#technologies)      | Outils et frameworks utilis√©s                    |
| [Installation](#installation)      | Pr√©-requis et configuration environnementale      |
| [Usage](#usage)                    | Ex√©cution du pipeline ETL et d√©ploiement           |
| [Structure du Projet](#structure-du-projet)  | Organisation des fichiers et dossiers            |
| [Configuration](#configuration)    | Param√©trage des connexions et variables d'environnement |
| [Journalisation](#journalisation)  | Gestion des logs                                   |
| [Contributeurs](#contributeurs)    | Qui a contribu√© au projet                          |
| [Licence](#licence)                | Licence du projet                                  |

## Introduction

Ce projet vise √† :
- **Extraire** les donn√©es depuis une base PostgreSQL,  
- **Transformer** ces donn√©es en un format exploitable pour l'analyse (via un sch√©ma en √©toile),  
- **Charger** les donn√©es transform√©es dans Snowflake pour un reporting et une analyse avanc√©e.

L'usage de **Parquet** √† chaque √©tape garantit une conservation historique et une tra√ßabilit√© des donn√©es (architecture en m√©dallion).

## Architecture

L'architecture adopt√©e est √† la fois modulaire et performante. Voici un sch√©ma simplifi√© :
Diagramme de l'Architecture

```mermaid
graph TD
    A[PostgreSQL (Source)] --> B[Extraction (Bronze)]
    B --> C[Transformation (Silver)]
    C --> D[Chargement (Gold) dans Snowflake]
```

### Sch√©ma en √âtoile (Star Schema)

Les donn√©es sont organis√©es en :
- **Tables de Dimensions** (clients, v√©hicules, branches, dates, paiements)
- **Tables de Faits** (locations, factures, maintenances)

| Tables de Dimensions    | Tables de Faits           |
| ----------------------- | ------------------------- |
| `DIM_CLIENT`            | `FACT_LOCATION`         |
| `DIM_VEHICULE`          | `FACT_FACTURE`          |
| `DIM_BRANCH`            | `FACT_MAINTENANCE`      |
| `DIM_DATE`              |                           |
| `DIM_PAIEMENT` (Optionnel)|                           |


## Technologies

- **Extraction et Transformation :** Python, Pandas, Polars (optionnel), SQLAlchemy  
- **Stockage Interm√©diaire :** Format Parquet (Bronze, Silver, Gold)  
- **Chargement :** Snowflake via `snowflake-sqlalchemy`  
- **Journalisation :** module `logging` de Python  
- **Environnements & Gestion de Variables :** Dotenv

## Installation

### Pr√©requis

- Python 3.10+
- PostgreSQL (base source)
- Compte Snowflake et acc√®s appropri√©
- Environnement virtuel (optionnel mais recommand√©)

### Installation des d√©pendances

Ex√©cutez la commande suivante‚ÄØ:
```bash
pip install pandas pyarrow sqlalchemy snowflake-sqlalchemy python-dotenv
```

## Usage

### Configuration des Variables d'Environnement

Cr√©ez un fichier `.env` √† la racine du projet avec les param√®tres suivants‚ÄØ:
```dotenv
# PostgreSQL
POSTGRES_URL=postgresql://postgres:postgres@localhost:6543/rentcar

# Snowflake
SNOWFLAKE_ACCOUNT=<your_account>
SNOWFLAKE_USER=<your_user>
SNOWFLAKE_PASSWORD=<your_password>
SNOWFLAKE_DATABASE=LOCATION_ETL
SNOWFLAKE_SCHEMA=LOCATION
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_ROLE=<your_role>
```

### Ex√©cution du Pipeline ETL

Pour lancer le processus ETL, ex√©cutez :
```bash
python etl.py
```
Le script suit les √©tapes suivantes‚ÄØ:
1. **Extraction (Bronze) :** R√©cup√©ration des donn√©es depuis PostgreSQL et sauvegarde au format Parquet.
2. **Transformation (Silver) :** Transformation des donn√©es en sch√©ma en √©toile, cr√©ation des dimensions et des faits, sauvegarde en Parquet.
3. **Chargement (Gold) :** Importation des donn√©es dans Snowflake avec un contexte configur√© via SQL Worksheet.

Les logs sont enregistr√©s dans le dossier `logs` pour un suivi d√©taill√©.

## Structure du Projet

```plaintext
‚îú‚îÄ‚îÄ etl.py                   # Script principal ETL
‚îú‚îÄ‚îÄ .env                     # Fichier de configuration (√† cr√©er)
‚îú‚îÄ‚îÄ logs/                    # R√©pertoire des logs
‚îÇ   ‚îî‚îÄ‚îÄ etl.log              # Journalisation du pipeline
‚îú‚îÄ‚îÄ bronze/                  # Donn√©es brutes extraites (format Parquet)
‚îú‚îÄ‚îÄ silver/                  # Donn√©es transform√©es (format Parquet)
‚îú‚îÄ‚îÄ gold/                    # Donn√©es charg√©es dans Snowflake (format Parquet)
‚îú‚îÄ‚îÄ README.md                # Ce fichier Readme
‚îî‚îÄ‚îÄ requirements.txt         # Liste des d√©pendances
```

## Configuration

### Postgres

- **URL de Connexion :** d√©finie dans `.env` sous `POSTGRES_URL`
- **Tables Sources :** `Clients`, `Vehicles`, `Branches`, `Locations`, `Factures`, `Entretiens`

### Snowflake

- **Param√®tres de Connexion :** configur√©s via `SNOWFLAKE_CONN_PARAMS` dans le script ETL
- **Sch√©ma Cibl√© :** `LOCATION` dans la base `LOCATION_ETL`
- **Contexte d'Ex√©cution :**
  - Database : `LOCATION_ETL`
  - Warehouse : `COMPUTE_WH`
  - Sch√©ma : `LOCATION`

## Journalisation

Les logs d√©taill√©s sont g√©n√©r√©s et stock√©s dans le dossier `logs/etl.log`. Ils permettent de suivre l'ex√©cution, le succ√®s ou les erreurs des diff√©rentes √©tapes du pipeline ETL. Utilisez ce fichier pour d√©boguer et surveiller l'√©tat du pipeline.

## Contributeurs

- **Abraham KOLOBOE** ‚Äì Data Engineer & Data Scientist  
  [LinkedIn](https://www.linkedin.com/in/abraham-zacharie-koloboe-data-science-ia-generative-llms-machine-learning/)


## Licence

Ce projet est sous licence [MIT](LICENSE).


## Remarques Finales

- üí° **Conseil :** Pour des mises √† jour incr√©mentales ult√©rieures, consid√©rez l'ajout de m√©canismes de CDC (Change Data Capture) ou de batchs incr√©mentaux.
- üöÄ **Prochaines √©tapes :** Impl√©menter des dashboards interactifs directement dans Snowflake ou via un outil BI (Tableau, Power BI).


N'h√©sitez pas √† contribuer ou √† proposer des am√©liorations !

Happy ETL-ing! üöÄ‚ú®